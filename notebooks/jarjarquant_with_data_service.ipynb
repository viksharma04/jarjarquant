{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d2fab38",
   "metadata": {},
   "source": [
    "# Jarjarquant with DataService Integration\n",
    "\n",
    "This notebook demonstrates how to use the DataService with Jarjarquant for comprehensive financial data analysis. The DataService provides efficient access to financial data stored in Parquet files using DuckDB for fast columnar operations.\n",
    "\n",
    "## What you'll learn:\n",
    "- How to initialize and use the DataService\n",
    "- Filtering stocks by metadata criteria\n",
    "- Integrating DataService with Jarjarquant for technical analysis\n",
    "- Computing features and applying triple barrier labeling\n",
    "- Analyzing multiple assets and finding correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0f9ef4",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "First, let's import the necessary libraries and initialize our services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd18ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jarjarquant import Jarjarquant\n",
    "from jarjarquant.data_service import DataService\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set display options for better output\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17658a0b",
   "metadata": {},
   "source": [
    "## 1. Initialize DataService\n",
    "\n",
    "The DataService provides a centralized interface for querying financial data stored in Parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdab4464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the DataService\n",
    "ds = DataService(\"jarjarquant/sample_data/data/\")\n",
    "\n",
    "print(\"=== DataService Initialization ===\\n\")\n",
    "print(\"üìä DataService initialized successfully!\")\n",
    "print(f\"üìÅ Data path: {ds.data_path}\")\n",
    "print(f\"üìà Equities path: {ds.equities_path}\")\n",
    "\n",
    "# Check available tickers\n",
    "tickers = ds.list_available_tickers()\n",
    "print(f\"\\nüìã Total available tickers: {len(tickers)}\")\n",
    "print(f\"üîç Sample tickers: {tickers[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4b8413",
   "metadata": {},
   "source": [
    "## 2. Explore Available Data\n",
    "\n",
    "Let's explore what sectors and metadata are available in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c87315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get available sectors\n",
    "sectors = ds.get_sectors()\n",
    "print(\"üè¢ Available Sectors:\")\n",
    "for i, sector in enumerate(sectors, 1):\n",
    "    print(f\"   {i:2d}. {sector}\")\n",
    "\n",
    "# Get available analyst ratings\n",
    "ratings = ds.get_analyst_ratings()\n",
    "print(f\"\\n‚≠ê Available Analyst Ratings: {ratings}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e1869a",
   "metadata": {},
   "source": [
    "## 3. Smart Stock Selection Using Metadata\n",
    "\n",
    "Instead of manually picking stocks, let's use metadata to find high-quality technology stocks with strong fundamentals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bfa1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Finding large-cap tech stocks with strong buy ratings...\\n\")\n",
    "\n",
    "# Try to get stocks with strong buy ratings\n",
    "tech_stocks = ds.get_sample_by_criteria(\n",
    "    n_samples=5,\n",
    "    sector=\"Technology services\",\n",
    "    min_market_cap=1e11,  # 100 billion minimum market cap\n",
    "    analyst_rating=\"Strong buy\",\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "if not tech_stocks:\n",
    "    print(\"‚ö†Ô∏è  No stocks found with 'Strong buy' rating. Trying with broader criteria...\")\n",
    "    # Fallback: get any large tech stocks\n",
    "    tech_stocks = ds.get_sample_by_criteria(\n",
    "        n_samples=5,\n",
    "        sector=\"Technology services\",\n",
    "        min_market_cap=5e10,  # 50 billion minimum\n",
    "        random_seed=42\n",
    "    )\n",
    "    \n",
    "if not tech_stocks:\n",
    "    # Final fallback to known tech stocks\n",
    "    tech_stocks = [\"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"META\"]\n",
    "    print(\"‚ö†Ô∏è  Using default tech stocks as fallback\")\n",
    "\n",
    "print(f\"‚úÖ Selected tickers: {tech_stocks}\")\n",
    "\n",
    "# Get metadata for selected stocks\n",
    "metadata = ds.get_metadata(tech_stocks)\n",
    "if not metadata.empty:\n",
    "    print(\"\\nüìä Stock Metadata:\")\n",
    "    display_cols = ['Description', 'Sector', 'Market capitalization']\n",
    "    if 'Analyst Rating' in metadata.columns:\n",
    "        display_cols.append('Analyst Rating')\n",
    "    print(metadata[display_cols])\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No metadata available for selected stocks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e984d0f5",
   "metadata": {},
   "source": [
    "## 4. Load Price Data for Analysis\n",
    "\n",
    "Now let's load historical price data for our selected stock to perform technical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6869b63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define analysis period\n",
    "start_date = \"2023-01-01\"\n",
    "end_date = \"2024-12-31\"\n",
    "\n",
    "# Get data for the first ticker\n",
    "primary_ticker = tech_stocks[0]\n",
    "print(f\"üìà Loading price data for {primary_ticker}...\")\n",
    "\n",
    "price_data = ds.get_price_data(\n",
    "    primary_ticker,\n",
    "    start_date=start_date,\n",
    "    end_date=end_date\n",
    ")\n",
    "\n",
    "if price_data.empty:\n",
    "    print(f\"‚ùå No data available for {primary_ticker}\")\n",
    "else:\n",
    "    print(f\"‚úÖ Loaded {len(price_data)} days of data for {primary_ticker}\")\n",
    "    print(f\"üìÖ Date range: {price_data.index.min()} to {price_data.index.max()}\")\n",
    "    \n",
    "    # Display basic statistics\n",
    "    print(\"\\nüìä Price Data Summary:\")\n",
    "    print(price_data[['Open', 'High', 'Low', 'Close', 'Volume']].describe())\n",
    "    \n",
    "    # Show recent data\n",
    "    print(\"\\nüìà Recent Price Data (last 5 days):\")\n",
    "    print(price_data[['Open', 'High', 'Low', 'Close', 'Volume']].tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6a202b",
   "metadata": {},
   "source": [
    "## 5. Price Visualization\n",
    "\n",
    "Let's create some visualizations to understand the price movements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d614e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not price_data.empty:\n",
    "    # Create subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "    \n",
    "    # Price chart\n",
    "    ax1.plot(price_data.index, price_data['Close'], label='Close Price', linewidth=2)\n",
    "    ax1.fill_between(price_data.index, price_data['Low'], price_data['High'], \n",
    "                     alpha=0.3, label='High-Low Range')\n",
    "    ax1.set_title(f'{primary_ticker} Price Chart', fontsize=14, fontweight='bold')\n",
    "    ax1.set_ylabel('Price ($)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Volume chart\n",
    "    ax2.bar(price_data.index, price_data['Volume'], alpha=0.7, color='orange')\n",
    "    ax2.set_title(f'{primary_ticker} Volume', fontsize=14, fontweight='bold')\n",
    "    ax2.set_ylabel('Volume')\n",
    "    ax2.set_xlabel('Date')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate and display basic metrics\n",
    "    returns = price_data['Close'].pct_change().dropna()\n",
    "    \n",
    "    print(f\"\\nüìà Performance Metrics for {primary_ticker}:\")\n",
    "    print(f\"   üìä Total Return: {((price_data['Close'].iloc[-1] / price_data['Close'].iloc[0]) - 1) * 100:.2f}%\")\n",
    "    print(f\"   üìä Average Daily Return: {returns.mean() * 100:.4f}%\")\n",
    "    print(f\"   üìä Volatility (Daily): {returns.std() * 100:.4f}%\")\n",
    "    print(f\"   üìä Sharpe Ratio (Daily): {returns.mean() / returns.std():.4f}\")\n",
    "    print(f\"   üìä Max Drawdown: {((price_data['Close'] / price_data['Close'].cummax()) - 1).min() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ec04e3",
   "metadata": {},
   "source": [
    "## 6. Initialize Jarjarquant for Technical Analysis\n",
    "\n",
    "Now let's use Jarjarquant to compute technical indicators and features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48823b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not price_data.empty:\n",
    "    print(\"üîß Initializing Jarjarquant for technical analysis...\\n\")\n",
    "    \n",
    "    # Convert to the format expected by Jarjarquant\n",
    "    jq_data = price_data[['Open', 'High', 'Low', 'Close', 'Volume']].copy()\n",
    "    \n",
    "    # Initialize Jarjarquant\n",
    "    jq = Jarjarquant(data=jq_data)\n",
    "    \n",
    "    print(f\"‚úÖ Jarjarquant initialized with {len(jq_data)} data points\")\n",
    "    print(f\"üìä Data columns: {list(jq_data.columns)}\")\n",
    "    print(f\"üìÖ Analysis period: {jq_data.index.min()} to {jq_data.index.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742fd1c4",
   "metadata": {},
   "source": [
    "## 7. Compute Technical Features\n",
    "\n",
    "Let's compute a comprehensive set of technical indicators and features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad4cc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not price_data.empty:\n",
    "    print(\"‚öôÔ∏è  Computing technical indicators and features...\\n\")\n",
    "    \n",
    "    # Compute features\n",
    "    features = jq.compute_features()\n",
    "    \n",
    "    print(f\"‚úÖ Generated {len(features.columns)} features\")\n",
    "    print(f\"üìä Feature shape: {features.shape}\")\n",
    "    print(f\"üìÖ Feature date range: {features.index.min()} to {features.index.max()}\")\n",
    "    \n",
    "    # Display feature categories\n",
    "    feature_names = list(features.columns)\n",
    "    print(f\"\\nüè∑Ô∏è  Feature categories (first 10): {feature_names[:10]}\")\n",
    "    \n",
    "    # Show feature statistics\n",
    "    print(\"\\nüìà Feature Summary Statistics:\")\n",
    "    print(features.describe().round(4))\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_counts = features.isnull().sum()\n",
    "    features_with_missing = missing_counts[missing_counts > 0]\n",
    "    \n",
    "    if len(features_with_missing) > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  Features with missing values: {len(features_with_missing)}\")\n",
    "        print(features_with_missing.head(10))\n",
    "    else:\n",
    "        print(\"\\n‚úÖ No missing values in features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6603507",
   "metadata": {},
   "source": [
    "## 8. Apply Triple Barrier Labeling\n",
    "\n",
    "The triple barrier method is a sophisticated approach to create labels for machine learning that considers profit-taking, stop-loss, and time-based exits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbc84d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not price_data.empty:\n",
    "    print(\"üéØ Applying triple barrier labeling...\\n\")\n",
    "    \n",
    "    # Apply triple barrier labeling\n",
    "    labels = jq.get_labels(\n",
    "        pt=0.02,  # 2% profit target\n",
    "        sl=0.01,  # 1% stop loss\n",
    "        horizon=20  # 20 day horizon\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Generated {len(labels)} labels\")\n",
    "    print(f\"üìä Label date range: {labels.index.min()} to {labels.index.max()}\")\n",
    "    \n",
    "    # Show label distribution\n",
    "    print(\"\\nüìä Label Distribution:\")\n",
    "    label_counts = labels.value_counts().sort_index()\n",
    "    label_percentages = (label_counts / len(labels) * 100).round(2)\n",
    "    \n",
    "    for label, count in label_counts.items():\n",
    "        percentage = label_percentages[label]\n",
    "        label_name = {-1: \"üìâ Sell\", 0: \"‚è∏Ô∏è  Hold\", 1: \"üìà Buy\"}.get(label, f\"Label {label}\")\n",
    "        print(f\"   {label_name}: {count:,} ({percentage}%)\")\n",
    "    \n",
    "    # Visualize label distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Create a bar plot\n",
    "    colors = ['red', 'gray', 'green']\n",
    "    label_names = ['Sell (-1)', 'Hold (0)', 'Buy (1)']\n",
    "    \n",
    "    bars = plt.bar(label_names, label_counts.values, color=colors, alpha=0.7)\n",
    "    plt.title(f'Triple Barrier Label Distribution for {primary_ticker}', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xlabel('Label')\n",
    "    \n",
    "    # Add percentage labels on bars\n",
    "    for bar, percentage in zip(bars, label_percentages.values):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                f'{percentage}%', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71a1247",
   "metadata": {},
   "source": [
    "## 9. Feature Importance Analysis\n",
    "\n",
    "Let's evaluate which technical features are most predictive of future price movements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2132c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not price_data.empty and len(labels) > 0:\n",
    "    print(\"üîç Evaluating feature importance...\\n\")\n",
    "    \n",
    "    # Evaluate features\n",
    "    importance = jq.evaluate_features(labels)\n",
    "    \n",
    "    if importance is not None and not importance.empty:\n",
    "        print(f\"‚úÖ Feature evaluation completed for {len(importance)} features\")\n",
    "        \n",
    "        # Get top features\n",
    "        top_features = importance.nlargest(15, 'importance')\n",
    "        \n",
    "        print(\"\\nüèÜ Top 15 Most Important Features:\")\n",
    "        for i, (feature_name, row) in enumerate(top_features.iterrows(), 1):\n",
    "            print(f\"   {i:2d}. {feature_name:<25}: {row['importance']:.6f}\")\n",
    "        \n",
    "        # Visualize feature importance\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Horizontal bar plot for better readability\n",
    "        y_pos = np.arange(len(top_features))\n",
    "        plt.barh(y_pos, top_features['importance'].values, alpha=0.8)\n",
    "        plt.yticks(y_pos, top_features.index)\n",
    "        plt.xlabel('Feature Importance Score')\n",
    "        plt.title(f'Top 15 Feature Importance for {primary_ticker}', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "        plt.gca().invert_yaxis()  # Highest importance at top\n",
    "        plt.grid(True, alpha=0.3, axis='x')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Show summary statistics of importance scores\n",
    "        print(\"\\nüìä Feature Importance Statistics:\")\n",
    "        print(importance['importance'].describe().round(6))\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Could not compute feature importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a84cb4a",
   "metadata": {},
   "source": [
    "## 10. Multi-Asset Analysis\n",
    "\n",
    "Now let's analyze multiple stocks to compare their characteristics and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201c1f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Analyzing multiple assets...\\n\")\n",
    "\n",
    "results = {}\n",
    "analysis_tickers = tech_stocks[:5]  # Analyze first 5 tickers\n",
    "\n",
    "for i, ticker in enumerate(analysis_tickers, 1):\n",
    "    print(f\"   {i}/{len(analysis_tickers)} Processing {ticker}...\", end=\" \")\n",
    "    \n",
    "    try:\n",
    "        # Get price data\n",
    "        ticker_data = ds.get_price_data(\n",
    "            ticker,\n",
    "            start_date=start_date,\n",
    "            end_date=end_date\n",
    "        )\n",
    "        \n",
    "        if ticker_data.empty:\n",
    "            print(\"‚ùå No data\")\n",
    "            continue\n",
    "        \n",
    "        # Get metadata\n",
    "        metadata = ds.get_metadata([ticker])\n",
    "        if not metadata.empty:\n",
    "            sector = metadata.loc[ticker, 'Sector']\n",
    "            market_cap = metadata.loc[ticker, 'Market capitalization']\n",
    "        else:\n",
    "            sector = \"Unknown\"\n",
    "            market_cap = 0\n",
    "        \n",
    "        # Calculate performance metrics\n",
    "        returns = ticker_data['Close'].pct_change().dropna()\n",
    "        \n",
    "        # Calculate maximum drawdown\n",
    "        rolling_max = ticker_data['Close'].cummax()\n",
    "        drawdown = (ticker_data['Close'] / rolling_max - 1)\n",
    "        max_drawdown = drawdown.min()\n",
    "        \n",
    "        # Calculate total return\n",
    "        total_return = (ticker_data['Close'].iloc[-1] / ticker_data['Close'].iloc[0]) - 1\n",
    "        \n",
    "        results[ticker] = {\n",
    "            'sector': sector,\n",
    "            'market_cap_b': market_cap / 1e9,  # Convert to billions\n",
    "            'total_return': total_return,\n",
    "            'avg_daily_return': returns.mean(),\n",
    "            'volatility': returns.std(),\n",
    "            'sharpe_ratio': returns.mean() / returns.std() if returns.std() > 0 else 0,\n",
    "            'max_drawdown': max_drawdown,\n",
    "            'data_points': len(ticker_data)\n",
    "        }\n",
    "        \n",
    "        print(\"‚úÖ\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {str(e)[:50]}...\")\n",
    "        continue\n",
    "\n",
    "# Display results\n",
    "if results:\n",
    "    results_df = pd.DataFrame(results).T\n",
    "    \n",
    "    print(\"\\nüìä Multi-Asset Analysis Results:\")\n",
    "    \n",
    "    # Format the dataframe for better display\n",
    "    display_df = results_df.copy()\n",
    "    display_df['total_return'] = (display_df['total_return'] * 100).round(2)\n",
    "    display_df['avg_daily_return'] = (display_df['avg_daily_return'] * 100).round(4)\n",
    "    display_df['volatility'] = (display_df['volatility'] * 100).round(4)\n",
    "    display_df['sharpe_ratio'] = display_df['sharpe_ratio'].round(4)\n",
    "    display_df['max_drawdown'] = (display_df['max_drawdown'] * 100).round(2)\n",
    "    display_df['market_cap_b'] = display_df['market_cap_b'].round(1)\n",
    "    \n",
    "    # Rename columns for better display\n",
    "    display_df.columns = ['Sector', 'Market Cap (B)', 'Total Return (%)', \n",
    "                         'Avg Daily Return (%)', 'Volatility (%)', \n",
    "                         'Sharpe Ratio', 'Max Drawdown (%)', 'Data Points']\n",
    "    \n",
    "    print(display_df)\n",
    "    \n",
    "    # Create performance comparison chart\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Total returns\n",
    "    ax1.bar(results_df.index, results_df['total_return'] * 100, alpha=0.8)\n",
    "    ax1.set_title('Total Returns (%)', fontweight='bold')\n",
    "    ax1.set_ylabel('Return (%)')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Volatility\n",
    "    ax2.bar(results_df.index, results_df['volatility'] * 100, alpha=0.8, color='orange')\n",
    "    ax2.set_title('Daily Volatility (%)', fontweight='bold')\n",
    "    ax2.set_ylabel('Volatility (%)')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Sharpe Ratio\n",
    "    ax3.bar(results_df.index, results_df['sharpe_ratio'], alpha=0.8, color='green')\n",
    "    ax3.set_title('Sharpe Ratio', fontweight='bold')\n",
    "    ax3.set_ylabel('Sharpe Ratio')\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Max Drawdown\n",
    "    ax4.bar(results_df.index, results_df['max_drawdown'] * 100, alpha=0.8, color='red')\n",
    "    ax4.set_title('Maximum Drawdown (%)', fontweight='bold')\n",
    "    ax4.set_ylabel('Drawdown (%)')\n",
    "    ax4.tick_params(axis='x', rotation=45)\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No analysis results available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bc5824",
   "metadata": {},
   "source": [
    "## 11. Correlation Analysis\n",
    "\n",
    "Let's analyze how these assets move together by computing return correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9df2b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîó Analyzing asset correlations...\\n\")\n",
    "\n",
    "if len(analysis_tickers) >= 2:\n",
    "    # Get returns for multiple assets\n",
    "    returns_data = {}\n",
    "    \n",
    "    for ticker in analysis_tickers:\n",
    "        print(f\"   Loading returns for {ticker}...\", end=\" \")\n",
    "        \n",
    "        ticker_data = ds.get_price_data(\n",
    "            ticker,\n",
    "            start_date=start_date,\n",
    "            end_date=end_date,\n",
    "            columns=['Close']\n",
    "        )\n",
    "        \n",
    "        if not ticker_data.empty:\n",
    "            returns_data[ticker] = ticker_data['Close'].pct_change()\n",
    "            print(\"‚úÖ\")\n",
    "        else:\n",
    "            print(\"‚ùå\")\n",
    "    \n",
    "    if len(returns_data) >= 2:\n",
    "        # Create returns dataframe\n",
    "        returns_df = pd.DataFrame(returns_data).dropna()\n",
    "        \n",
    "        print(f\"\\nüìä Correlation analysis with {len(returns_df)} common observations\")\n",
    "        \n",
    "        # Calculate correlation matrix\n",
    "        correlation = returns_df.corr()\n",
    "        \n",
    "        print(\"\\nüîó Return Correlation Matrix:\")\n",
    "        print(correlation.round(3))\n",
    "        \n",
    "        # Visualize correlation matrix\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        # Create heatmap\n",
    "        mask = np.triu(np.ones_like(correlation, dtype=bool))  # Mask upper triangle\n",
    "        sns.heatmap(correlation, mask=mask, annot=True, cmap='RdYlBu_r', \n",
    "                   center=0, square=True, linewidths=0.5, \n",
    "                   cbar_kws={\"shrink\": .8}, fmt='.3f')\n",
    "        \n",
    "        plt.title('Asset Return Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Find highest and lowest correlations\n",
    "        # Get upper triangle of correlation matrix (excluding diagonal)\n",
    "        upper_tri = correlation.where(np.triu(np.ones(correlation.shape), k=1).astype(bool))\n",
    "        \n",
    "        # Find highest correlation pair\n",
    "        max_corr = upper_tri.max().max()\n",
    "        max_pair = upper_tri.stack().idxmax()\n",
    "        \n",
    "        # Find lowest correlation pair\n",
    "        min_corr = upper_tri.min().min()\n",
    "        min_pair = upper_tri.stack().idxmin()\n",
    "        \n",
    "        print(f\"\\nüîó Correlation Insights:\")\n",
    "        print(f\"   üìà Highest correlation: {max_pair[0]} - {max_pair[1]} ({max_corr:.3f})\")\n",
    "        print(f\"   üìâ Lowest correlation: {min_pair[0]} - {min_pair[1]} ({min_corr:.3f})\")\n",
    "        \n",
    "        # Calculate average correlation\n",
    "        avg_corr = upper_tri.stack().mean()\n",
    "        print(f\"   üìä Average correlation: {avg_corr:.3f}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå Not enough valid return data for correlation analysis\")\n",
    "else:\n",
    "    print(\"‚ùå Need at least 2 tickers for correlation analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4381709b",
   "metadata": {},
   "source": [
    "## 12. Summary and Key Insights\n",
    "\n",
    "Let's summarize what we've learned from this comprehensive analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17d655f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìã ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nüéØ Analysis Focus:\")\n",
    "print(f\"   üìä Primary Asset: {primary_ticker}\")\n",
    "print(f\"   üìÖ Period: {start_date} to {end_date}\")\n",
    "print(f\"   üè¢ Sector Focus: Technology Services\")\n",
    "print(f\"   üìà Assets Analyzed: {len(analysis_tickers)}\")\n",
    "\n",
    "if not price_data.empty:\n",
    "    print(f\"\\nüí° Key Findings:\")\n",
    "    \n",
    "    # Price performance\n",
    "    total_return = ((price_data['Close'].iloc[-1] / price_data['Close'].iloc[0]) - 1) * 100\n",
    "    returns = price_data['Close'].pct_change().dropna()\n",
    "    volatility = returns.std() * np.sqrt(252) * 100  # Annualized\n",
    "    sharpe = returns.mean() / returns.std() * np.sqrt(252)  # Annualized\n",
    "    \n",
    "    print(f\"   üìà {primary_ticker} total return: {total_return:.2f}%\")\n",
    "    print(f\"   üìä {primary_ticker} annualized volatility: {volatility:.2f}%\")\n",
    "    print(f\"   ‚≠ê {primary_ticker} annualized Sharpe ratio: {sharpe:.3f}\")\n",
    "    \n",
    "    if 'features' in locals():\n",
    "        print(f\"   üîß Technical features generated: {len(features.columns)}\")\n",
    "    \n",
    "    if 'labels' in locals():\n",
    "        label_counts = labels.value_counts()\n",
    "        if 1 in label_counts and -1 in label_counts:\n",
    "            buy_sell_ratio = label_counts[1] / label_counts[-1]\n",
    "            print(f\"   üéØ Buy/Sell signal ratio: {buy_sell_ratio:.2f}\")\n",
    "    \n",
    "    if 'importance' in locals() and importance is not None:\n",
    "        top_feature = importance.nlargest(1, 'importance').index[0]\n",
    "        top_score = importance.nlargest(1, 'importance')['importance'].iloc[0]\n",
    "        print(f\"   üèÜ Most predictive feature: {top_feature} ({top_score:.6f})\")\n",
    "\n",
    "print(f\"\\nüõ†Ô∏è  DataService Capabilities Demonstrated:\")\n",
    "print(f\"   ‚úÖ Smart stock selection using metadata criteria\")\n",
    "print(f\"   ‚úÖ Efficient price data loading with date filtering\")\n",
    "print(f\"   ‚úÖ Multi-asset analysis and comparison\")\n",
    "print(f\"   ‚úÖ Integration with Jarjarquant for technical analysis\")\n",
    "print(f\"   ‚úÖ Feature engineering and importance evaluation\")\n",
    "print(f\"   ‚úÖ Triple barrier labeling for ML applications\")\n",
    "print(f\"   ‚úÖ Correlation analysis across assets\")\n",
    "\n",
    "print(f\"\\nüöÄ Next Steps:\")\n",
    "print(f\"   üìö Explore different sectors and market conditions\")\n",
    "print(f\"   üî¨ Experiment with different labeling parameters\")\n",
    "print(f\"   ü§ñ Build machine learning models using the features\")\n",
    "print(f\"   üìà Implement backtesting strategies\")\n",
    "print(f\"   üîÑ Automate the analysis pipeline\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 50)\n",
    "print(f\"‚úÖ Analysis completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b8a410",
   "metadata": {},
   "source": [
    "## 13. Cleanup\n",
    "\n",
    "Don't forget to properly close the DataService connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4d1bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the DataService connection\n",
    "ds.close()\n",
    "print(\"üîí DataService connection closed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e404c45f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated the powerful integration between Jarjarquant's DataService and the main quantitative analysis framework. We've shown how to:\n",
    "\n",
    "1. **Intelligently select stocks** using metadata criteria rather than manual selection\n",
    "2. **Efficiently load and analyze** large datasets using DuckDB's columnar operations\n",
    "3. **Generate comprehensive technical features** using Jarjarquant's feature engineering\n",
    "4. **Apply sophisticated labeling techniques** with triple barrier methods\n",
    "5. **Evaluate feature importance** to identify the most predictive signals\n",
    "6. **Compare multiple assets** across various performance metrics\n",
    "7. **Analyze correlations** to understand asset relationships\n",
    "\n",
    "The DataService provides a robust foundation for quantitative analysis, enabling researchers and practitioners to focus on strategy development rather than data management complexities.\n",
    "\n",
    "### Key Benefits:\n",
    "- **Performance**: Fast queries using DuckDB's columnar engine\n",
    "- **Flexibility**: Easy filtering and selection using metadata\n",
    "- **Integration**: Seamless connection with Jarjarquant's analysis tools\n",
    "- **Scalability**: Handles large datasets efficiently\n",
    "- **Reproducibility**: Consistent data access patterns\n",
    "\n",
    "Feel free to modify the parameters, explore different sectors, or extend the analysis to suit your specific research needs!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
