{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d2fab38",
   "metadata": {},
   "source": [
    "# Jarjarquant with DataService Integration\n",
    "\n",
    "This notebook demonstrates how to use the DataService with Jarjarquant for comprehensive financial data analysis. The DataService provides efficient access to financial data stored in Parquet files using DuckDB for fast columnar operations.\n",
    "\n",
    "## What you'll learn:\n",
    "- How to initialize and use the DataService\n",
    "- Filtering stocks by metadata criteria\n",
    "- Integrating DataService with Jarjarquant for technical analysis\n",
    "- Computing features and applying triple barrier labeling\n",
    "- Analyzing multiple assets and finding correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0f9ef4",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "First, let's import the necessary libraries and initialize our services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd18ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jarjarquant import Jarjarquant\n",
    "from jarjarquant.data_service import DataService\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set display options for better output\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17658a0b",
   "metadata": {},
   "source": [
    "## 1. Initialize DataService\n",
    "\n",
    "The DataService provides a centralized interface for querying financial data stored in Parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdab4464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the DataService\n",
    "ds = DataService(\"jarjarquant/sample_data/data/\")\n",
    "\n",
    "print(\"=== DataService Initialization ===\\n\")\n",
    "print(\"ğŸ“Š DataService initialized successfully!\")\n",
    "print(f\"ğŸ“ Data path: {ds.data_path}\")\n",
    "print(f\"ğŸ“ˆ Equities path: {ds.equities_path}\")\n",
    "\n",
    "# Check available tickers\n",
    "tickers = ds.list_available_tickers()\n",
    "print(f\"\\nğŸ“‹ Total available tickers: {len(tickers)}\")\n",
    "print(f\"ğŸ” Sample tickers: {tickers[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4b8413",
   "metadata": {},
   "source": [
    "## 2. Explore Available Data\n",
    "\n",
    "Let's explore what sectors and metadata are available in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c87315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get available sectors\n",
    "sectors = ds.get_sectors()\n",
    "print(\"ğŸ¢ Available Sectors:\")\n",
    "for i, sector in enumerate(sectors, 1):\n",
    "    print(f\"   {i:2d}. {sector}\")\n",
    "\n",
    "# Get available analyst ratings\n",
    "ratings = ds.get_analyst_ratings()\n",
    "print(f\"\\nâ­ Available Analyst Ratings: {ratings}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e1869a",
   "metadata": {},
   "source": [
    "## 3. Smart Stock Selection Using Metadata\n",
    "\n",
    "Instead of manually picking stocks, let's use metadata to find high-quality technology stocks with strong fundamentals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bfa1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ” Finding large-cap tech stocks with strong buy ratings...\\n\")\n",
    "\n",
    "# Try to get stocks with strong buy ratings\n",
    "tech_stocks = ds.get_sample_by_criteria(\n",
    "    n_samples=5,\n",
    "    sector=\"Technology services\",\n",
    "    min_market_cap=1e11,  # 100 billion minimum market cap\n",
    "    analyst_rating=\"Strong buy\",\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "if not tech_stocks:\n",
    "    print(\"âš ï¸  No stocks found with 'Strong buy' rating. Trying with broader criteria...\")\n",
    "    # Fallback: get any large tech stocks\n",
    "    tech_stocks = ds.get_sample_by_criteria(\n",
    "        n_samples=5,\n",
    "        sector=\"Technology services\",\n",
    "        min_market_cap=5e10,  # 50 billion minimum\n",
    "        random_seed=42\n",
    "    )\n",
    "    \n",
    "if not tech_stocks:\n",
    "    # Final fallback to known tech stocks\n",
    "    tech_stocks = [\"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"META\"]\n",
    "    print(\"âš ï¸  Using default tech stocks as fallback\")\n",
    "\n",
    "print(f\"âœ… Selected tickers: {tech_stocks}\")\n",
    "\n",
    "# Get metadata for selected stocks\n",
    "metadata = ds.get_metadata(tech_stocks)\n",
    "if not metadata.empty:\n",
    "    print(\"\\nğŸ“Š Stock Metadata:\")\n",
    "    display_cols = ['Description', 'Sector', 'Market capitalization']\n",
    "    if 'Analyst Rating' in metadata.columns:\n",
    "        display_cols.append('Analyst Rating')\n",
    "    print(metadata[display_cols])\n",
    "else:\n",
    "    print(\"âš ï¸  No metadata available for selected stocks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e984d0f5",
   "metadata": {},
   "source": [
    "## 4. Load Price Data for Analysis\n",
    "\n",
    "Now let's load historical price data for our selected stock to perform technical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6869b63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define analysis period\n",
    "start_date = \"2023-01-01\"\n",
    "end_date = \"2024-12-31\"\n",
    "\n",
    "# Get data for the first ticker\n",
    "primary_ticker = tech_stocks[0]\n",
    "print(f\"ğŸ“ˆ Loading price data for {primary_ticker}...\")\n",
    "\n",
    "price_data = ds.get_price_data(\n",
    "    primary_ticker,\n",
    "    start_date=start_date,\n",
    "    end_date=end_date\n",
    ")\n",
    "\n",
    "if price_data.empty:\n",
    "    print(f\"âŒ No data available for {primary_ticker}\")\n",
    "else:\n",
    "    print(f\"âœ… Loaded {len(price_data)} days of data for {primary_ticker}\")\n",
    "    print(f\"ğŸ“… Date range: {price_data.index.min()} to {price_data.index.max()}\")\n",
    "    \n",
    "    # Display basic statistics\n",
    "    print(\"\\nğŸ“Š Price Data Summary:\")\n",
    "    print(price_data[['Open', 'High', 'Low', 'Close', 'Volume']].describe())\n",
    "    \n",
    "    # Show recent data\n",
    "    print(\"\\nğŸ“ˆ Recent Price Data (last 5 days):\")\n",
    "    print(price_data[['Open', 'High', 'Low', 'Close', 'Volume']].tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6a202b",
   "metadata": {},
   "source": [
    "## 5. Price Visualization\n",
    "\n",
    "Let's create some visualizations to understand the price movements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d614e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not price_data.empty:\n",
    "    # Create subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "    \n",
    "    # Price chart\n",
    "    ax1.plot(price_data.index, price_data['Close'], label='Close Price', linewidth=2)\n",
    "    ax1.fill_between(price_data.index, price_data['Low'], price_data['High'], \n",
    "                     alpha=0.3, label='High-Low Range')\n",
    "    ax1.set_title(f'{primary_ticker} Price Chart', fontsize=14, fontweight='bold')\n",
    "    ax1.set_ylabel('Price ($)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Volume chart\n",
    "    ax2.bar(price_data.index, price_data['Volume'], alpha=0.7, color='orange')\n",
    "    ax2.set_title(f'{primary_ticker} Volume', fontsize=14, fontweight='bold')\n",
    "    ax2.set_ylabel('Volume')\n",
    "    ax2.set_xlabel('Date')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate and display basic metrics\n",
    "    returns = price_data['Close'].pct_change().dropna()\n",
    "    \n",
    "    print(f\"\\nğŸ“ˆ Performance Metrics for {primary_ticker}:\")\n",
    "    print(f\"   ğŸ“Š Total Return: {((price_data['Close'].iloc[-1] / price_data['Close'].iloc[0]) - 1) * 100:.2f}%\")\n",
    "    print(f\"   ğŸ“Š Average Daily Return: {returns.mean() * 100:.4f}%\")\n",
    "    print(f\"   ğŸ“Š Volatility (Daily): {returns.std() * 100:.4f}%\")\n",
    "    print(f\"   ğŸ“Š Sharpe Ratio (Daily): {returns.mean() / returns.std():.4f}\")\n",
    "    print(f\"   ğŸ“Š Max Drawdown: {((price_data['Close'] / price_data['Close'].cummax()) - 1).min() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ec04e3",
   "metadata": {},
   "source": [
    "## 6. Initialize Jarjarquant for Technical Analysis\n",
    "\n",
    "Now let's use Jarjarquant to compute technical indicators and features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48823b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not price_data.empty:\n",
    "    print(\"ğŸ”§ Initializing Jarjarquant for technical analysis...\\n\")\n",
    "    \n",
    "    # Convert to the format expected by Jarjarquant\n",
    "    jq_data = price_data[['Open', 'High', 'Low', 'Close', 'Volume']].copy()\n",
    "    \n",
    "    # Initialize Jarjarquant\n",
    "    jq = Jarjarquant(data=jq_data)\n",
    "    \n",
    "    print(f\"âœ… Jarjarquant initialized with {len(jq_data)} data points\")\n",
    "    print(f\"ğŸ“Š Data columns: {list(jq_data.columns)}\")\n",
    "    print(f\"ğŸ“… Analysis period: {jq_data.index.min()} to {jq_data.index.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742fd1c4",
   "metadata": {},
   "source": [
    "## 7. Compute Technical Features\n",
    "\n",
    "Let's compute a comprehensive set of technical indicators and features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad4cc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not price_data.empty:\n",
    "    print(\"âš™ï¸  Computing technical indicators and features...\\n\")\n",
    "    \n",
    "    # Compute features\n",
    "    features = jq.compute_features()\n",
    "    \n",
    "    print(f\"âœ… Generated {len(features.columns)} features\")\n",
    "    print(f\"ğŸ“Š Feature shape: {features.shape}\")\n",
    "    print(f\"ğŸ“… Feature date range: {features.index.min()} to {features.index.max()}\")\n",
    "    \n",
    "    # Display feature categories\n",
    "    feature_names = list(features.columns)\n",
    "    print(f\"\\nğŸ·ï¸  Feature categories (first 10): {feature_names[:10]}\")\n",
    "    \n",
    "    # Show feature statistics\n",
    "    print(\"\\nğŸ“ˆ Feature Summary Statistics:\")\n",
    "    print(features.describe().round(4))\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_counts = features.isnull().sum()\n",
    "    features_with_missing = missing_counts[missing_counts > 0]\n",
    "    \n",
    "    if len(features_with_missing) > 0:\n",
    "        print(f\"\\nâš ï¸  Features with missing values: {len(features_with_missing)}\")\n",
    "        print(features_with_missing.head(10))\n",
    "    else:\n",
    "        print(\"\\nâœ… No missing values in features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6603507",
   "metadata": {},
   "source": [
    "## 8. Apply Triple Barrier Labeling\n",
    "\n",
    "The triple barrier method is a sophisticated approach to create labels for machine learning that considers profit-taking, stop-loss, and time-based exits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbc84d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not price_data.empty:\n",
    "    print(\"ğŸ¯ Applying triple barrier labeling...\\n\")\n",
    "    \n",
    "    # Apply triple barrier labeling\n",
    "    labels = jq.get_labels(\n",
    "        pt=0.02,  # 2% profit target\n",
    "        sl=0.01,  # 1% stop loss\n",
    "        horizon=20  # 20 day horizon\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Generated {len(labels)} labels\")\n",
    "    print(f\"ğŸ“Š Label date range: {labels.index.min()} to {labels.index.max()}\")\n",
    "    \n",
    "    # Show label distribution\n",
    "    print(\"\\nğŸ“Š Label Distribution:\")\n",
    "    label_counts = labels.value_counts().sort_index()\n",
    "    label_percentages = (label_counts / len(labels) * 100).round(2)\n",
    "    \n",
    "    for label, count in label_counts.items():\n",
    "        percentage = label_percentages[label]\n",
    "        label_name = {-1: \"ğŸ“‰ Sell\", 0: \"â¸ï¸  Hold\", 1: \"ğŸ“ˆ Buy\"}.get(label, f\"Label {label}\")\n",
    "        print(f\"   {label_name}: {count:,} ({percentage}%)\")\n",
    "    \n",
    "    # Visualize label distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Create a bar plot\n",
    "    colors = ['red', 'gray', 'green']\n",
    "    label_names = ['Sell (-1)', 'Hold (0)', 'Buy (1)']\n",
    "    \n",
    "    bars = plt.bar(label_names, label_counts.values, color=colors, alpha=0.7)\n",
    "    plt.title(f'Triple Barrier Label Distribution for {primary_ticker}', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xlabel('Label')\n",
    "    \n",
    "    # Add percentage labels on bars\n",
    "    for bar, percentage in zip(bars, label_percentages.values):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                f'{percentage}%', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71a1247",
   "metadata": {},
   "source": [
    "## 9. Feature Importance Analysis\n",
    "\n",
    "Let's evaluate which technical features are most predictive of future price movements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2132c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not price_data.empty and len(labels) > 0:\n",
    "    print(\"ğŸ” Evaluating feature importance...\\n\")\n",
    "    \n",
    "    # Evaluate features\n",
    "    importance = jq.evaluate_features(labels)\n",
    "    \n",
    "    if importance is not None and not importance.empty:\n",
    "        print(f\"âœ… Feature evaluation completed for {len(importance)} features\")\n",
    "        \n",
    "        # Get top features\n",
    "        top_features = importance.nlargest(15, 'importance')\n",
    "        \n",
    "        print(\"\\nğŸ† Top 15 Most Important Features:\")\n",
    "        for i, (feature_name, row) in enumerate(top_features.iterrows(), 1):\n",
    "            print(f\"   {i:2d}. {feature_name:<25}: {row['importance']:.6f}\")\n",
    "        \n",
    "        # Visualize feature importance\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Horizontal bar plot for better readability\n",
    "        y_pos = np.arange(len(top_features))\n",
    "        plt.barh(y_pos, top_features['importance'].values, alpha=0.8)\n",
    "        plt.yticks(y_pos, top_features.index)\n",
    "        plt.xlabel('Feature Importance Score')\n",
    "        plt.title(f'Top 15 Feature Importance for {primary_ticker}', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "        plt.gca().invert_yaxis()  # Highest importance at top\n",
    "        plt.grid(True, alpha=0.3, axis='x')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Show summary statistics of importance scores\n",
    "        print(\"\\nğŸ“Š Feature Importance Statistics:\")\n",
    "        print(importance['importance'].describe().round(6))\n",
    "        \n",
    "    else:\n",
    "        print(\"âš ï¸  Could not compute feature importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a84cb4a",
   "metadata": {},
   "source": [
    "## 10. Multi-Asset Analysis\n",
    "\n",
    "Now let's analyze multiple stocks to compare their characteristics and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201c1f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“Š Analyzing multiple assets...\\n\")\n",
    "\n",
    "results = {}\n",
    "analysis_tickers = tech_stocks[:5]  # Analyze first 5 tickers\n",
    "\n",
    "for i, ticker in enumerate(analysis_tickers, 1):\n",
    "    print(f\"   {i}/{len(analysis_tickers)} Processing {ticker}...\", end=\" \")\n",
    "    \n",
    "    try:\n",
    "        # Get price data\n",
    "        ticker_data = ds.get_price_data(\n",
    "            ticker,\n",
    "            start_date=start_date,\n",
    "            end_date=end_date\n",
    "        )\n",
    "        \n",
    "        if ticker_data.empty:\n",
    "            print(\"âŒ No data\")\n",
    "            continue\n",
    "        \n",
    "        # Get metadata\n",
    "        metadata = ds.get_metadata([ticker])\n",
    "        if not metadata.empty:\n",
    "            sector = metadata.loc[ticker, 'Sector']\n",
    "            market_cap = metadata.loc[ticker, 'Market capitalization']\n",
    "        else:\n",
    "            sector = \"Unknown\"\n",
    "            market_cap = 0\n",
    "        \n",
    "        # Calculate performance metrics\n",
    "        returns = ticker_data['Close'].pct_change().dropna()\n",
    "        \n",
    "        # Calculate maximum drawdown\n",
    "        rolling_max = ticker_data['Close'].cummax()\n",
    "        drawdown = (ticker_data['Close'] / rolling_max - 1)\n",
    "        max_drawdown = drawdown.min()\n",
    "        \n",
    "        # Calculate total return\n",
    "        total_return = (ticker_data['Close'].iloc[-1] / ticker_data['Close'].iloc[0]) - 1\n",
    "        \n",
    "        results[ticker] = {\n",
    "            'sector': sector,\n",
    "            'market_cap_b': market_cap / 1e9,  # Convert to billions\n",
    "            'total_return': total_return,\n",
    "            'avg_daily_return': returns.mean(),\n",
    "            'volatility': returns.std(),\n",
    "            'sharpe_ratio': returns.mean() / returns.std() if returns.std() > 0 else 0,\n",
    "            'max_drawdown': max_drawdown,\n",
    "            'data_points': len(ticker_data)\n",
    "        }\n",
    "        \n",
    "        print(\"âœ…\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {str(e)[:50]}...\")\n",
    "        continue\n",
    "\n",
    "# Display results\n",
    "if results:\n",
    "    results_df = pd.DataFrame(results).T\n",
    "    \n",
    "    print(\"\\nğŸ“Š Multi-Asset Analysis Results:\")\n",
    "    \n",
    "    # Format the dataframe for better display\n",
    "    display_df = results_df.copy()\n",
    "    display_df['total_return'] = (display_df['total_return'] * 100).round(2)\n",
    "    display_df['avg_daily_return'] = (display_df['avg_daily_return'] * 100).round(4)\n",
    "    display_df['volatility'] = (display_df['volatility'] * 100).round(4)\n",
    "    display_df['sharpe_ratio'] = display_df['sharpe_ratio'].round(4)\n",
    "    display_df['max_drawdown'] = (display_df['max_drawdown'] * 100).round(2)\n",
    "    display_df['market_cap_b'] = display_df['market_cap_b'].round(1)\n",
    "    \n",
    "    # Rename columns for better display\n",
    "    display_df.columns = ['Sector', 'Market Cap (B)', 'Total Return (%)', \n",
    "                         'Avg Daily Return (%)', 'Volatility (%)', \n",
    "                         'Sharpe Ratio', 'Max Drawdown (%)', 'Data Points']\n",
    "    \n",
    "    print(display_df)\n",
    "    \n",
    "    # Create performance comparison chart\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Total returns\n",
    "    ax1.bar(results_df.index, results_df['total_return'] * 100, alpha=0.8)\n",
    "    ax1.set_title('Total Returns (%)', fontweight='bold')\n",
    "    ax1.set_ylabel('Return (%)')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Volatility\n",
    "    ax2.bar(results_df.index, results_df['volatility'] * 100, alpha=0.8, color='orange')\n",
    "    ax2.set_title('Daily Volatility (%)', fontweight='bold')\n",
    "    ax2.set_ylabel('Volatility (%)')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Sharpe Ratio\n",
    "    ax3.bar(results_df.index, results_df['sharpe_ratio'], alpha=0.8, color='green')\n",
    "    ax3.set_title('Sharpe Ratio', fontweight='bold')\n",
    "    ax3.set_ylabel('Sharpe Ratio')\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Max Drawdown\n",
    "    ax4.bar(results_df.index, results_df['max_drawdown'] * 100, alpha=0.8, color='red')\n",
    "    ax4.set_title('Maximum Drawdown (%)', fontweight='bold')\n",
    "    ax4.set_ylabel('Drawdown (%)')\n",
    "    ax4.tick_params(axis='x', rotation=45)\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ No analysis results available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bc5824",
   "metadata": {},
   "source": [
    "## 11. Correlation Analysis\n",
    "\n",
    "Let's analyze how these assets move together by computing return correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9df2b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”— Analyzing asset correlations...\\n\")\n",
    "\n",
    "if len(analysis_tickers) >= 2:\n",
    "    # Get returns for multiple assets\n",
    "    returns_data = {}\n",
    "    \n",
    "    for ticker in analysis_tickers:\n",
    "        print(f\"   Loading returns for {ticker}...\", end=\" \")\n",
    "        \n",
    "        ticker_data = ds.get_price_data(\n",
    "            ticker,\n",
    "            start_date=start_date,\n",
    "            end_date=end_date,\n",
    "            columns=['Close']\n",
    "        )\n",
    "        \n",
    "        if not ticker_data.empty:\n",
    "            returns_data[ticker] = ticker_data['Close'].pct_change()\n",
    "            print(\"âœ…\")\n",
    "        else:\n",
    "            print(\"âŒ\")\n",
    "    \n",
    "    if len(returns_data) >= 2:\n",
    "        # Create returns dataframe\n",
    "        returns_df = pd.DataFrame(returns_data).dropna()\n",
    "        \n",
    "        print(f\"\\nğŸ“Š Correlation analysis with {len(returns_df)} common observations\")\n",
    "        \n",
    "        # Calculate correlation matrix\n",
    "        correlation = returns_df.corr()\n",
    "        \n",
    "        print(\"\\nğŸ”— Return Correlation Matrix:\")\n",
    "        print(correlation.round(3))\n",
    "        \n",
    "        # Visualize correlation matrix\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        # Create heatmap\n",
    "        mask = np.triu(np.ones_like(correlation, dtype=bool))  # Mask upper triangle\n",
    "        sns.heatmap(correlation, mask=mask, annot=True, cmap='RdYlBu_r', \n",
    "                   center=0, square=True, linewidths=0.5, \n",
    "                   cbar_kws={\"shrink\": .8}, fmt='.3f')\n",
    "        \n",
    "        plt.title('Asset Return Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Find highest and lowest correlations\n",
    "        # Get upper triangle of correlation matrix (excluding diagonal)\n",
    "        upper_tri = correlation.where(np.triu(np.ones(correlation.shape), k=1).astype(bool))\n",
    "        \n",
    "        # Find highest correlation pair\n",
    "        max_corr = upper_tri.max().max()\n",
    "        max_pair = upper_tri.stack().idxmax()\n",
    "        \n",
    "        # Find lowest correlation pair\n",
    "        min_corr = upper_tri.min().min()\n",
    "        min_pair = upper_tri.stack().idxmin()\n",
    "        \n",
    "        print(f\"\\nğŸ”— Correlation Insights:\")\n",
    "        print(f\"   ğŸ“ˆ Highest correlation: {max_pair[0]} - {max_pair[1]} ({max_corr:.3f})\")\n",
    "        print(f\"   ğŸ“‰ Lowest correlation: {min_pair[0]} - {min_pair[1]} ({min_corr:.3f})\")\n",
    "        \n",
    "        # Calculate average correlation\n",
    "        avg_corr = upper_tri.stack().mean()\n",
    "        print(f\"   ğŸ“Š Average correlation: {avg_corr:.3f}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"âŒ Not enough valid return data for correlation analysis\")\n",
    "else:\n",
    "    print(\"âŒ Need at least 2 tickers for correlation analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4381709b",
   "metadata": {},
   "source": [
    "## 12. Summary and Key Insights\n",
    "\n",
    "Let's summarize what we've learned from this comprehensive analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17d655f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“‹ ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nğŸ¯ Analysis Focus:\")\n",
    "print(f\"   ğŸ“Š Primary Asset: {primary_ticker}\")\n",
    "print(f\"   ğŸ“… Period: {start_date} to {end_date}\")\n",
    "print(f\"   ğŸ¢ Sector Focus: Technology Services\")\n",
    "print(f\"   ğŸ“ˆ Assets Analyzed: {len(analysis_tickers)}\")\n",
    "\n",
    "if not price_data.empty:\n",
    "    print(f\"\\nğŸ’¡ Key Findings:\")\n",
    "    \n",
    "    # Price performance\n",
    "    total_return = ((price_data['Close'].iloc[-1] / price_data['Close'].iloc[0]) - 1) * 100\n",
    "    returns = price_data['Close'].pct_change().dropna()\n",
    "    volatility = returns.std() * np.sqrt(252) * 100  # Annualized\n",
    "    sharpe = returns.mean() / returns.std() * np.sqrt(252)  # Annualized\n",
    "    \n",
    "    print(f\"   ğŸ“ˆ {primary_ticker} total return: {total_return:.2f}%\")\n",
    "    print(f\"   ğŸ“Š {primary_ticker} annualized volatility: {volatility:.2f}%\")\n",
    "    print(f\"   â­ {primary_ticker} annualized Sharpe ratio: {sharpe:.3f}\")\n",
    "    \n",
    "    if 'features' in locals():\n",
    "        print(f\"   ğŸ”§ Technical features generated: {len(features.columns)}\")\n",
    "    \n",
    "    if 'labels' in locals():\n",
    "        label_counts = labels.value_counts()\n",
    "        if 1 in label_counts and -1 in label_counts:\n",
    "            buy_sell_ratio = label_counts[1] / label_counts[-1]\n",
    "            print(f\"   ğŸ¯ Buy/Sell signal ratio: {buy_sell_ratio:.2f}\")\n",
    "    \n",
    "    if 'importance' in locals() and importance is not None:\n",
    "        top_feature = importance.nlargest(1, 'importance').index[0]\n",
    "        top_score = importance.nlargest(1, 'importance')['importance'].iloc[0]\n",
    "        print(f\"   ğŸ† Most predictive feature: {top_feature} ({top_score:.6f})\")\n",
    "\n",
    "print(f\"\\nğŸ› ï¸  DataService Capabilities Demonstrated:\")\n",
    "print(f\"   âœ… Smart stock selection using metadata criteria\")\n",
    "print(f\"   âœ… Efficient price data loading with date filtering\")\n",
    "print(f\"   âœ… Multi-asset analysis and comparison\")\n",
    "print(f\"   âœ… Integration with Jarjarquant for technical analysis\")\n",
    "print(f\"   âœ… Feature engineering and importance evaluation\")\n",
    "print(f\"   âœ… Triple barrier labeling for ML applications\")\n",
    "print(f\"   âœ… Correlation analysis across assets\")\n",
    "\n",
    "print(f\"\\nğŸš€ Next Steps:\")\n",
    "print(f\"   ğŸ“š Explore different sectors and market conditions\")\n",
    "print(f\"   ğŸ”¬ Experiment with different labeling parameters\")\n",
    "print(f\"   ğŸ¤– Build machine learning models using the features\")\n",
    "print(f\"   ğŸ“ˆ Implement backtesting strategies\")\n",
    "print(f\"   ğŸ”„ Automate the analysis pipeline\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 50)\n",
    "print(f\"âœ… Analysis completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b8a410",
   "metadata": {},
   "source": [
    "## 13. Cleanup\n",
    "\n",
    "Don't forget to properly close the DataService connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4d1bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the DataService connection\n",
    "ds.close()\n",
    "print(\"ğŸ”’ DataService connection closed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e404c45f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated the powerful integration between Jarjarquant's DataService and the main quantitative analysis framework. We've shown how to:\n",
    "\n",
    "1. **Intelligently select stocks** using metadata criteria rather than manual selection\n",
    "2. **Efficiently load and analyze** large datasets using DuckDB's columnar operations\n",
    "3. **Generate comprehensive technical features** using Jarjarquant's feature engineering\n",
    "4. **Apply sophisticated labeling techniques** with triple barrier methods\n",
    "5. **Evaluate feature importance** to identify the most predictive signals\n",
    "6. **Compare multiple assets** across various performance metrics\n",
    "7. **Analyze correlations** to understand asset relationships\n",
    "\n",
    "The DataService provides a robust foundation for quantitative analysis, enabling researchers and practitioners to focus on strategy development rather than data management complexities.\n",
    "\n",
    "### Key Benefits:\n",
    "- **Performance**: Fast queries using DuckDB's columnar engine\n",
    "- **Flexibility**: Easy filtering and selection using metadata\n",
    "- **Integration**: Seamless connection with Jarjarquant's analysis tools\n",
    "- **Scalability**: Handles large datasets efficiently\n",
    "- **Reproducibility**: Consistent data access patterns\n",
    "\n",
    "Feel free to modify the parameters, explore different sectors, or extend the analysis to suit your specific research needs!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
